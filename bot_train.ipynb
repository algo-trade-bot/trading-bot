{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters to tune:**\n",
    "- Learning Rate: Controls how much the model's weights are updated during training. A higher learning rate might lead to faster learning but can cause instability. A lower learning rate ensures more stable but slower learning.\n",
    "- N_steps: Number of steps the agent takes before updating its policy. It's a trade-off between performance and memory usage. In a trading environment, this could be aligned with the frequency of decision-making.\n",
    "- Gamma (Discount Factor): Determines the importance of future rewards. A lower value makes the agent short-sighted by discounting future rewards heavily.\n",
    "- Gae_lambda (Generalized Advantage Estimator): Balances bias and variance in the advantage estimation. It affects how the agent evaluates the trade-off between immediate and future rewards.\n",
    "- Ent_coef (Entropy Coefficient): Encourages exploration by adding an entropy bonus to the objective function. Higher entropy can help explore more strategies in a complex trading market.\n",
    "- Seed: Sets the random seed for reproducibility of training results.\n",
    "- Use_sde (Stochastic Differential Equations): If enabled, introduces stochasticity in the policy, which can help exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ETHUSD_5.csv', header=None, names=['Time', 'Open', 'High', 'Low', 'Close', 'Volume', 'Trades'])\n",
    "df['Time'] = pd.to_datetime(df['Time'], unit='s')\n",
    "df.set_index('Time', inplace=True)\n",
    "df = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "env = TradeEnv(6, 288, 1000, df)\n",
    "model = RecurrentPPO(\"MlpLstmPolicy\", env, verbose=1, learning_rate=0.001, n_steps=32, gamma=0.65, ent_coef=0.05, seed=42)\n",
    "model.learn(10000)\n",
    "\n",
    "# model.save(\"ppo_recurrent\")\n",
    "# del model # remove to demonstrate saving and loading\n",
    "\n",
    "# model = RecurrentPPO.load(\"ppo_recurrent\")\n",
    "\n",
    "# obs = vec_env.reset()\n",
    "# # cell and hidden state of the LSTM\n",
    "# lstm_states = None\n",
    "# num_envs = 1\n",
    "# # Episode start signals are used to reset the lstm states\n",
    "# episode_starts = np.ones((num_envs,), dtype=bool)\n",
    "# while True:\n",
    "#     action, lstm_states = model.predict(obs, state=lstm_states, episode_start=episode_starts, deterministic=True)\n",
    "#     obs, rewards, dones, info = vec_env.step(action)\n",
    "#     episode_starts = dones\n",
    "#     vec_env.render(\"human\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datalibs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
